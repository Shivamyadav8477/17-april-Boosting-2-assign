{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b060b8-96c0-4aa0-bdd6-fddb82b0e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b12ec-9f5f-410b-8b7d-c252a6257792",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regression, often referred to as Gradient Boosting Machines (GBM), is a powerful and popular machine learning technique used for regression and classification tasks. It's an ensemble learning method that builds a predictive model by combining the predictions of multiple weaker models called decision trees. Gradient Boosting Regression aims to minimize the loss function (typically a mean squared error for regression tasks) by iteratively adding decision trees to the ensemble.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialization:** The process begins with a simple model, often an initial prediction based on the mean or median of the target variable for regression tasks. This initial prediction is often referred to as the \"first guess.\"\n",
    "\n",
    "2. **Gradient Descent:** The algorithm then focuses on the errors or residuals from the first guess. It calculates the negative gradient (or slope) of the loss function with respect to these residuals. This gradient represents the direction and magnitude of the steepest decrease in the loss function.\n",
    "\n",
    "3. **Building Weak Models (Decision Trees):** Gradient Boosting Regression uses decision trees as weak learners. A shallow decision tree with only a few nodes is typically used. These decision trees are often referred to as \"stumps\" because of their limited depth.\n",
    "\n",
    "4. **Fitting Weak Models:** The algorithm fits a decision tree to the negative gradient (residuals) of the loss function. This decision tree aims to capture the patterns or relationships in the residuals that the previous model did not.\n",
    "\n",
    "5. **Updating Predictions:** The predictions from the newly created decision tree are scaled by a learning rate (a small positive value) and added to the previous predictions. This step updates the model's predictions to reduce the overall loss.\n",
    "\n",
    "6. **Iterative Process:** Steps 2 to 5 are repeated iteratively, with each new decision tree focusing on the residuals from the previous predictions. The process continues until a predefined number of trees (a hyperparameter) is reached or until a stopping criterion, such as a minimum loss threshold, is met.\n",
    "\n",
    "7. **Final Ensemble Prediction:** The final prediction is obtained by combining the predictions from all the decision trees in the ensemble. The ensemble prediction is the sum of the initial prediction and the scaled contributions from each decision tree.\n",
    "\n",
    "Gradient Boosting Regression is known for its high predictive accuracy and ability to capture complex relationships in the data. It often outperforms other regression algorithms when appropriately tuned. However, it can be sensitive to hyperparameters, such as the learning rate and tree depth, and may require careful tuning to avoid overfitting. Popular implementations of Gradient Boosting Regression include GradientBoostingRegressor in Scikit-Learn and XGBoost, LightGBM, and CatBoost libraries, which offer optimized and efficient versions of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59302b-2c9d-4ab3-ab2b-8fa02d789d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acfdf6d-a9f7-4773-b5bb-bcfd6abc73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mplementing a gradient boosting algorithm from scratch is a complex task, but I can provide you with a simplified example of gradient boosting for regression using Python and NumPy. In practice, you'd typically use established libraries like Scikit-Learn, XGBoost, or LightGBM for real-world applications.\n",
    "\n",
    "Here's a simplified Python code snippet to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad43f9b-dbd7-4e62-b08f-f230a66371d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 18.50\n",
      "R-squared: -9.51\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a simple dataset\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([2, 4, 5, 4.5, 6])\n",
    "\n",
    "# Define hyperparameters\n",
    "n_estimators = 100  # Number of boosting iterations\n",
    "learning_rate = 0.1  # Step size for updates\n",
    "max_depth = 1  # Maximum depth of each decision tree (stump)\n",
    "\n",
    "# Initialize predictions with the mean of y\n",
    "predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "# Create a list to store the decision trees\n",
    "trees = []\n",
    "\n",
    "# Gradient Boosting\n",
    "for _ in range(n_estimators):\n",
    "    # Calculate residuals\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Create a decision tree regressor\n",
    "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    \n",
    "    # Fit the tree to the residuals\n",
    "    tree.fit(X, residuals)\n",
    "    \n",
    "    # Make predictions with the tree\n",
    "    tree_predictions = tree.predict(X)\n",
    "    \n",
    "    # Update predictions with a scaled version of the tree's predictions\n",
    "    predictions += learning_rate * tree_predictions\n",
    "    \n",
    "    # Append the tree to the list\n",
    "    trees.append(tree)\n",
    "\n",
    "# Final ensemble prediction\n",
    "final_predictions = np.sum([learning_rate * tree.predict(X) for tree in trees], axis=0)\n",
    "\n",
    "# Calculate mean squared error and R-squared\n",
    "mse = mean_squared_error(y, final_predictions)\n",
    "r2 = r2_score(y, final_predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1743df59-3c99-4b74-8ad5-6855762434e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this simplified example:\n",
    "\n",
    "We create a simple dataset with one feature (X) and the target variable (y).\n",
    "We set hyperparameters such as the number of boosting iterations (n_estimators), learning rate (learning_rate), and maximum depth of the decision trees (max_depth).\n",
    "We initialize the predictions with the mean of y.\n",
    "We iteratively fit decision tree regressors to the residuals and update the predictions.\n",
    "Finally, we calculate the mean squared error and R-squared to evaluate the model's performance.\n",
    "Keep in mind that this is a basic example for educational purposes. Real-world implementations often involve more sophisticated techniques and optimizations. Popular libraries like Scikit-Learn provide efficient implementations of gradient boosting for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b26267b-d721-44ff-a563-26798c6f8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a103c5-e4d5-4b54-b9a0-bbe64f531693",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! To experiment with different hyperparameters and optimize the performance of the gradient boosting model, you can use grid search or random search. Here's how you can perform grid search using Scikit-Learn's GridSearchCV to find the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949b995-0494-444d-84d7-f7061b9cb577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_mse = -grid_search.best_score_  # Since we used negative mean squared error\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_gb_regressor = GradientBoostingRegressor(**best_params)\n",
    "final_gb_regressor.fit(X, y)\n",
    "\n",
    "# Evaluate the final model\n",
    "final_predictions = final_gb_regressor.predict(X)\n",
    "final_mse = mean_squared_error(y, final_predictions)\n",
    "final_r2 = r2_score(y, final_predictions)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Best Mean Squared Error: {best_mse:.2f}\")\n",
    "print(f\"Final Mean Squared Error: {final_mse:.2f}\")\n",
    "print(f\"Final R-squared: {final_r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9fe8a-5fdc-42fa-abae-2cd0df93842a",
   "metadata": {},
   "outputs": [],
   "source": [
    " this code:\n",
    "\n",
    "We define a parameter grid with different values for n_estimators, learning_rate, and max_depth.\n",
    "We create a GridSearchCV object with the gradient boosting regressor, the parameter grid, the scoring metric (negative mean squared error in this case), and the number of cross-validation folds (cv).\n",
    "We fit the GridSearchCV object to the data to find the best hyperparameters.\n",
    "We extract the best hyperparameters and the best mean squared error from the grid search results.\n",
    "Finally, we train the final gradient boosting model with the best hyperparameters and evaluate its performance.\n",
    "You can adjust the parameter grid and scoring metric to suit your specific regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf932c-61ab-4a65-a71a-fb1d019c7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a95db-d802-42dd-a1ad-8bc284514ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Gradient Boosting, a weak learner, also known as a base learner or base estimator, is a machine learning model that performs slightly better than random guessing but is not a strong predictive model on its own. Weak learners are typically simple models, such as decision stumps (decision trees with a single split), shallow decision trees, or linear models.\n",
    "\n",
    "The key characteristic of a weak learner is that its predictive performance is only slightly better than random chance, which means its accuracy is slightly better than 50% for binary classification problems.\n",
    "\n",
    "Gradient Boosting builds an ensemble model by sequentially adding weak learners to the ensemble, with each new learner focusing on the mistakes or residuals made by the previous learners. The combined effect of these weak learners, through a weighted sum or other aggregation technique, results in a strong and highly accurate predictive model.\n",
    "\n",
    "Gradient Boosting relies on the principle of boosting, where each weak learner is trained to correct the errors made by the previous ones. This iterative process continues until a predefined number of weak learners (or until a specified level of performance) is reached.\n",
    "\n",
    "The strength of Gradient Boosting lies in its ability to adapt and improve its predictions by giving more weight to the samples that are misclassified or have larger residuals, effectively reducing the bias and increasing the model's accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa0b5b-c0c4-49d8-acf9-3349ee498e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1fd789-f594-4d9e-b685-06f4fc2a4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1. **Sequential Improvement:** Gradient Boosting builds an ensemble model by combining the predictions of multiple weak learners (typically decision trees) sequentially. Each weak learner is trained to correct the errors made by the previous ones. This sequential process focuses on the mistakes of the ensemble, gradually reducing the model's bias and improving its accuracy.\n",
    "\n",
    "2. **Gradient Descent:** The algorithm uses gradient descent optimization to minimize a loss function. Initially, the loss function represents the difference between the actual target values and the predictions made by the current ensemble. In each iteration, a new weak learner is added to the ensemble to reduce the loss further. Gradient descent is used to find the optimal parameters (e.g., tree structure and leaf values) for the new learner.\n",
    "\n",
    "3. **Weighted Voting:** Weak learners are assigned weights based on their performance. Better-performing learners are given higher weights, meaning they have a stronger influence on the final predictions. This weighted voting ensures that more accurate models contribute more to the ensemble's decisions.\n",
    "\n",
    "4. **Adaptive Learning:** Gradient Boosting adapts to the data by adjusting its focus over time. It assigns higher importance to data points that are misclassified or have larger residuals. This adaptability allows the algorithm to capture complex relationships in the data and improve prediction accuracy.\n",
    "\n",
    "5. **Ensemble of Weak Models:** Despite individual learners being weak (i.e., having limited predictive power), their combined effect results in a strong model. The ensemble leverages the strengths of multiple models to make accurate predictions, effectively reducing both bias and variance.\n",
    "\n",
    "6. **Regularization:** Gradient Boosting includes regularization techniques to prevent overfitting. Regularization terms are added to the loss function to penalize complex models, controlling the depth and complexity of individual weak learners.\n",
    "\n",
    "7. **Tree Pruning:** The decision trees used as weak learners are often shallow to avoid overfitting. Pruning techniques are applied to limit tree depth and control complexity, making the individual trees more interpretable and less prone to overfitting.\n",
    "\n",
    "In summary, Gradient Boosting is an ensemble learning technique that incrementally improves the predictive performance of a model by combining the predictions of weak learners in a weighted manner, with a focus on correcting errors and minimizing a loss function. The adaptability, regularization, and sequential nature of the algorithm contribute to its effectiveness in capturing complex patterns in data and producing accurate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
